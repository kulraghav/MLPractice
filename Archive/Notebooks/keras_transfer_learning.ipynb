{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n",
    "\n",
    "input shape for VGG16\n",
    "https://stackoverflow.com/questions/41903051/change-input-tensor-shape-for-vgg16-application\n",
    "\n",
    "https://ai.stackexchange.com/questions/2928/keras-valueerror-error-when-checking-model-target-expected-activation-4-to hav\n",
    "\n",
    "ResourceExhaustedError\n",
    "https://stackoverflow.com/questions/42315289/resource-exhausted-memory-error-when-trying-to-train-a-keras-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "image_size = 224\n",
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7f0c714bd748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be6bd72b0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be6bf1198> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f0be6bf1c18> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be63a5240> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be6349cf8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f0be63632e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be637e128> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be637ed68> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be631b3c8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f0be633bba8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be62ef128> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be62efd68> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be628f3c8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f0be62acba8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be6263128> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be6263d68> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f0be62823c8> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f0be623c5f8> True\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers except the last k layers\n",
    "k = 4\n",
    "\n",
    "for layer in vgg_conv.layers[:-k]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 40,407,874\n",
      "Trainable params: 32,772,610\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    " \n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    " \n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 images belonging to 2 classes.\n",
      "Found 153 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = '../Data/hymenoptera_data/train/'\n",
    "validation_dir = '../Data/hymenoptera_data/val'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 20\n",
    "val_batchsize = 10\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "13/12 [===============================] - 8s 635ms/step - loss: 4.5990 - acc: 0.6468 - val_loss: 0.5359 - val_acc: 0.7843\n",
      "Epoch 2/30\n",
      "13/12 [===============================] - 5s 367ms/step - loss: 0.3896 - acc: 0.8196 - val_loss: 0.4195 - val_acc: 0.8497\n",
      "Epoch 3/30\n",
      "13/12 [===============================] - 5s 403ms/step - loss: 0.2741 - acc: 0.8686 - val_loss: 0.5339 - val_acc: 0.8301\n",
      "Epoch 4/30\n",
      "13/12 [===============================] - 5s 371ms/step - loss: 0.2770 - acc: 0.8980 - val_loss: 0.5477 - val_acc: 0.8105\n",
      "Epoch 5/30\n",
      "13/12 [===============================] - 5s 369ms/step - loss: 0.2466 - acc: 0.8968 - val_loss: 0.3516 - val_acc: 0.8824\n",
      "Epoch 6/30\n",
      "13/12 [===============================] - 5s 370ms/step - loss: 0.1862 - acc: 0.9382 - val_loss: 0.6090 - val_acc: 0.8235\n",
      "Epoch 7/30\n",
      "13/12 [===============================] - 5s 382ms/step - loss: 0.1488 - acc: 0.9459 - val_loss: 0.5185 - val_acc: 0.7908\n",
      "Epoch 8/30\n",
      "13/12 [===============================] - 5s 377ms/step - loss: 0.2812 - acc: 0.8841 - val_loss: 0.5134 - val_acc: 0.8693\n",
      "Epoch 9/30\n",
      "13/12 [===============================] - 5s 371ms/step - loss: 0.1189 - acc: 0.9536 - val_loss: 0.5887 - val_acc: 0.8366\n",
      "Epoch 10/30\n",
      "13/12 [===============================] - 5s 390ms/step - loss: 0.1531 - acc: 0.9536 - val_loss: 0.4698 - val_acc: 0.9020\n",
      "Epoch 11/30\n",
      "13/12 [===============================] - 5s 391ms/step - loss: 0.1781 - acc: 0.9536 - val_loss: 0.6520 - val_acc: 0.8366\n",
      "Epoch 12/30\n",
      "13/12 [===============================] - 5s 372ms/step - loss: 0.0445 - acc: 0.9845 - val_loss: 0.6386 - val_acc: 0.8693\n",
      "Epoch 13/30\n",
      "13/12 [===============================] - 5s 378ms/step - loss: 0.3300 - acc: 0.9046 - val_loss: 0.3980 - val_acc: 0.8497\n",
      "Epoch 14/30\n",
      "13/12 [===============================] - 5s 397ms/step - loss: 0.0745 - acc: 0.9652 - val_loss: 0.5710 - val_acc: 0.9020\n",
      "Epoch 15/30\n",
      "13/12 [===============================] - 5s 386ms/step - loss: 0.0206 - acc: 0.9961 - val_loss: 0.8415 - val_acc: 0.8497\n",
      "Epoch 16/30\n",
      "13/12 [===============================] - 5s 378ms/step - loss: 0.2245 - acc: 0.9498 - val_loss: 0.5068 - val_acc: 0.8758\n",
      "Epoch 17/30\n",
      "13/12 [===============================] - 5s 371ms/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.7447 - val_acc: 0.9020\n",
      "Epoch 18/30\n",
      "13/12 [===============================] - 5s 393ms/step - loss: 0.1308 - acc: 0.9459 - val_loss: 0.8461 - val_acc: 0.8366\n",
      "Epoch 19/30\n",
      "13/12 [===============================] - 5s 391ms/step - loss: 0.1457 - acc: 0.9730 - val_loss: 0.6148 - val_acc: 0.8301\n",
      "Epoch 20/30\n",
      "12/12 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9958"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "# Train the model\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)\n",
    " \n",
    "# Save the model\n",
    "model.save('small_last4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(tf_gpu) Python3",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/quora_duplicate_questions/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (404290, 6)\n",
      "columns = ['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n"
     ]
    }
   ],
   "source": [
    "print(\"df.shape = {}\".format(df.shape))\n",
    "print(\"columns = {}\".format(df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    255027\n",
       "1    149263\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    How many pairs are labeled as duplicate ?\n",
    "\"\"\"\n",
    "df['is_duplicate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_unique_qid1 = 290654 num_unique_qid2 = 299364\n",
      "num common qids = 52085\n",
      "total num_unique_qids = 537933\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    How many unique question ids ?\n",
    "    How many in the intersection of qid1 and qid2 ?\n",
    "    How many in the union of qid1 and qid2 ?\n",
    "\"\"\"\n",
    "print(\"num_unique_qid1 = {} num_unique_qid2 = {}\".format(df['qid1'].nunique(), df['qid2'].nunique()))\n",
    "print(\"num common qids = {}\".format(len(set(df['qid1']).intersection(set(df['qid2'])))))\n",
    "print(\"total num_unique_qids = {}\".format(len(set(df['qid1']).union(set(df['qid2'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['question1'] + \" \" + df['question2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features=10000-1).fit(df['text'])\n",
    "\n",
    "other_index = len(count_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokenizer = re.compile(count_vec.token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_seqs(texts, max_len=10):\n",
    "    seqs = texts.apply(lambda s: \n",
    "        [count_vec.vocabulary_[w] if w in count_vec.vocabulary_ else other_index\n",
    "         for w in words_tokenizer.findall(s.lower())])\n",
    "    return pad_sequences(seqs, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = \\\n",
    "    train_test_split(create_padded_seqs(df['question1']), \n",
    "                     create_padded_seqs(df['question2']),\n",
    "                     df['is_duplicate'].values,\n",
    "                     stratify=df['is_duplicate'].values,\n",
    "                     test_size=0.3, random_state=1989)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 10, 100)      1000000     input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          365568      embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 256)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           4112        multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            17          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,369,697\n",
      "Trainable params: 1,369,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.layers as layer\n",
    "from keras.models import Model\n",
    "\n",
    "input1_tensor = layer.Input(X1_train.shape[1:])\n",
    "input2_tensor = layer.Input(X2_train.shape[1:])\n",
    "\n",
    "words_embedding_layer = layer.Embedding(X1_train.max() + 1, 100)\n",
    "seq_embedding_layer = layer.LSTM(256, activation='tanh')\n",
    "\n",
    "seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))\n",
    "\n",
    "merge_layer = layer.multiply([seq_embedding(input1_tensor), seq_embedding(input2_tensor)])\n",
    "\n",
    "dense1_layer = layer.Dense(16, activation='sigmoid')(merge_layer)\n",
    "ouput_layer = layer.Dense(1, activation='sigmoid')(dense1_layer)\n",
    "\n",
    "model = Model([input1_tensor, input2_tensor], ouput_layer)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283000 samples, validate on 121287 samples\n",
      "Epoch 1/6\n",
      " - 289s - loss: 0.5081 - acc: 0.7541 - val_loss: 0.4718 - val_acc: 0.7777\n",
      "Epoch 2/6\n",
      " - 284s - loss: 0.4309 - acc: 0.8009 - val_loss: 0.4371 - val_acc: 0.7978\n",
      "Epoch 3/6\n",
      " - 285s - loss: 0.3798 - acc: 0.8290 - val_loss: 0.4261 - val_acc: 0.8070\n",
      "Epoch 4/6\n",
      " - 284s - loss: 0.3346 - acc: 0.8536 - val_loss: 0.4207 - val_acc: 0.8134\n",
      "Epoch 5/6\n",
      " - 282s - loss: 0.2920 - acc: 0.8754 - val_loss: 0.4291 - val_acc: 0.8178\n",
      "Epoch 6/6\n",
      " - 282s - loss: 0.2513 - acc: 0.8960 - val_loss: 0.4459 - val_acc: 0.8185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fafd3a784e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X1_train, X2_train], y_train, \n",
    "          validation_data=([X1_val, X2_val], y_val), \n",
    "          batch_size=128, epochs=6, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_model = Model([input1_tensor, input2_tensor], merge_layer)\n",
    "features_model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_train = features_model.predict([X1_train, X2_train], batch_size=128)\n",
    "F_val = features_model.predict([X1_val, X2_val], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.654243\tval-logloss:0.660035\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 10 rounds.\n",
      "[10]\ttrain-logloss:0.430273\tval-logloss:0.48743\n",
      "[20]\ttrain-logloss:0.348252\tval-logloss:0.43799\n",
      "[30]\ttrain-logloss:0.295311\tval-logloss:0.417279\n",
      "[40]\ttrain-logloss:0.266981\tval-logloss:0.411906\n",
      "[50]\ttrain-logloss:0.241197\tval-logloss:0.410289\n",
      "[60]\ttrain-logloss:0.224583\tval-logloss:0.411725\n",
      "Stopping. Best iteration:\n",
      "[51]\ttrain-logloss:0.239351\tval-logloss:0.410122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dTrain = xgb.DMatrix(F_train, label=y_train)\n",
    "dVal = xgb.DMatrix(F_val, label=y_val)\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1, \n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1 / F_train.shape[1]**0.5,\n",
    "    'min_child_weight': 5,\n",
    "    'silent': 1\n",
    "}\n",
    "bst = xgb.train(xgb_params, dTrain, 1000,  [(dTrain,'train'), (dVal,'val')], \n",
    "                verbose_eval=10, early_stopping_rounds=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question1'])\n",
    "X2_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question2'])\n",
    "\n",
    "F_test = features_model.predict([X1_test, X2_test], batch_size=128)\n",
    "\n",
    "dTest = xgb.DMatrix(F_test)\n",
    "\n",
    "df_sub = pd.DataFrame({\n",
    "        'test_id': df_all[df_all['test_id'].notnull()]['test_id'].values,\n",
    "        'is_duplicate': bst.predict(dTest, ntree_limit=bst.best_ntree_limit)\n",
    "    }).set_index('test_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(tf_gpu) Python3",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
